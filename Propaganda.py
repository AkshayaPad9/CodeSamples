"""Copy of 4740_FA20_p2_apr65_ak749.ipynb
Automatically generated by Colaboratory.
Original file is located at
    https://colab.research.google.com/drive/1rl7TV0I7peuDg-gPFhvgnDvdq7FnLAu-
    Created by Akshaya Raghavan and Aditi Kashyap
# Project 2: Span Identification with Sequence Labeling Models
# Overview
---
In this project, you will implement a model that identifies relevant information in a text and tags it with the appropriate label. Particularly, the task of this project is **Propaganda Identification**. The given dataset contains (manual) annotations indicating fragments of text that exhibit one of a set of well-known [propaganda techniques](https://propaganda.qcri.org/annotations/definitions.html). Your task is to develop NLP models to identify these propagandistic spans of text automatically. We will treat this as a **sequence-tagging task**: for each token in the input text, assign a label $y\in\{0,1\}$, such that *1 represents propagandistic text* and *0 represents non-propaganda*.   (A description of the original task formulation is [here](https://propaganda.qcri.org/ptc/).  We are working on a modified version of their "span identification" task.)
"""

from google.colab import drive
import os
drive.mount('/content/drive', force_remount=True)

train_root_path = os.path.join(os.getcwd(), "drive", "My Drive","Colab Notebooks")
train_path = os.path.join(train_root_path,"p2-cs4740-propaganda/train/train")
test_root_path = os.path.join(os.getcwd(), "drive", "My Drive","Colab Notebooks")
test_path = os.path.join(test_root_path,"p2-cs4740-propaganda/test/test")
print(os.listdir(train_path)[:5])
print(os.listdir(test_path)[:5])

import os
from typing import List, Tuple

from nltk import word_tokenize, sent_tokenize
import nltk
nltk.download('punkt')

#makes the documents into strings 
def read_txt(fname):
  with open(fname) as open_article:
    lines = open_article.read()
  return lines

def read_labels(labels : str) -> List[Tuple[int, int]]:
	"processing of labels file"
	labels = labels.split("\n")[:-1]
	labels = [tuple(map(int, l.split("\t")[1:])) for l in labels]
	return labels

def sort_and_merge_labels(labels : List[Tuple[int, int]]) -> List[Tuple[int, int]]:
  "sort labels, necessary for later splitting"
  if len(labels) == 0:
    return labels
  labels = list(sorted(labels, key = lambda t: t[0]))
  # merge
  curr = labels[0]
  merged = []
  for l in labels[1:]:
      # if distinct, add
      if l[0] > curr[1]:
        merged.append(curr)
        curr = l
      # else merge
      else:
        curr = (curr[0], max(curr[1], l[1]))
  merged.append(curr)
  return merged

def split_with_labels(labels : List[Tuple[int, int]], article : str) -> Tuple[List[str], List[int]]:
  "split text into segments based upon labels"
  if len(labels) == 0:
    return [article], [0]
  segments = []
  binary_class = []
  start = 0
  for l_start, l_end in labels:
    std_seg = article[start:l_start]
    prop_seg = article[l_start:l_end]
    segments.append(std_seg)
    binary_class.append(0)
    segments.append(prop_seg)
    binary_class.append(1)
    start = l_end
  last_seg = article[start:]
  segments.append(last_seg)
  binary_class.append(0)
  return segments, binary_class

def remove_newline_fix_punc_seg(segments):
  " preprocessing necessry for tokenization to be consistent"
  segments = [s.replace("\n", " ").replace(".", " .") for s in segments]
  return segments

def remove_newline_fix_punc_art(article):
  " preprocessing necessry for tokenization to be consistent"
  article = article.replace("\n", " ").replace(".", " .")
  return article

def get_toks(input):
  output = []
  for toks in [list(map(str.lower, word_tokenize(sent))) for sent in sent_tokenize(input)]:
    output += toks
  return output

# This is the function you may need to call
def tokenize_article(article_file):
  "calls all functions above and perform sanity checks"
  article = read_txt(article_file)
  article = remove_newline_fix_punc_art(article)
  art_toks = get_toks(article)
  return art_toks

# This is the function you may need to call
def master_tokenizer(article_file, labels_file):
  "calls all functions above and perform sanity checks"
	# read and get labels
  article = read_txt(article_file)
  labels = read_txt(labels_file)
  labels = read_labels(labels)
  labels = sort_and_merge_labels(labels)
  segments, binary_class = split_with_labels(labels, article)
  article = remove_newline_fix_punc_art(article)
  segments = remove_newline_fix_punc_seg(segments)
  # sanity check
  reconstructed = ""
  for seg, lab in zip(segments, binary_class):
    reconstructed += seg
  assert reconstructed == article
	# tokenize
  seg_toks = []
  new_labels = []
  for seg, label in zip(segments, binary_class):
    new_toks = get_toks(seg)
    seg_toks += new_toks
    new_labels += [label for _ in range(len(new_toks))]
	# sanity check
  art_toks = get_toks(article)
  sanity = True
  if len(art_toks) != len(seg_toks):
    sanity = False
  for i, (at, st, lab) in enumerate(zip(art_toks, seg_toks, new_labels)):
    if at != st:
      sanity = False
      break
  return seg_toks, new_labels, sanity

""" 6. Execute the commands below to visualize the tokenization:"""

article_file = "article698018235.txt"
labels_file = "article698018235.task-si.labels"
article_file = os.path.join(train_path, article_file)
labels_file = os.path.join(train_path, labels_file)
tokens, labels,_ = master_tokenizer(article_file, labels_file)

print(len(tokens), len(labels), tokens[:50])

"""7.  Provide some quantitative data exploration. Assess dataset size, documents lengths and class inbalance. Give some examples of sentences containing propaganda techniques."""

# code here
full_list_token_label_training=[]
for article in os.listdir(train_path):
  article_split = article.rsplit(".")
  if (article_split[1]=="txt"):
      article_file = os.path.join(train_path,article)
      labels_file = article_split[0] + ".task-si.labels"
      labels_file = os.path.join(train_path,labels_file)
      full_list_token_label_training.append(master_tokenizer(article_file, labels_file))

print(full_list_token_label_training[1])

#Class imbalance is measured to see amount of propaganda in training vs non
#propaganda
from collections import Counter
def class_imbalance(token_labels):
  for token_label in token_labels:
    #counter within 2nd element (label) 
    counter_imbalance = Counter(token_label[1])
    return counter_imbalance

class_imbalance(full_list_token_label_training)

doc_lengths = []
for token_label in full_list_token_label_training:
      doc_lengths.append(len(token_label[0]))

print(min(doc_lengths))
print(max(doc_lengths))

small_article_token = doc_lengths.index(120)
large_article_token = doc_lengths.index(8305)

imbalance_small_article = Counter(full_list_token_label_training[small_article_token][1])
imbalance_large_article = Counter(full_list_token_label_training[large_article_token][1])

print(imbalance_small_article)
print(imbalance_large_article)

"""# Model 1: HMM Implementation"""

from random import sample
length = len(full_list_token_label_training)
validation_length=int(length*.1)
training_length=int(length*.9)
list_token_label_training=full_list_token_label_training[0:training_length]
list_token_label_validation = sample(full_list_token_label_training,validation_length)
print(len(list_token_label_validation))

import random
def create_unknown_word_list(lst):
  modified_list = []
  vocabulary=set()
  for token in lst:
    if token in vocabulary:
      modified_list.append(token)
    else:
      r = random.randint(1,101)
      if (r<30):
        modified_list.append("<UNK>")
      else:
        modified_list.append(token)
      vocabulary.add(token)
  return modified_list,vocabulary

from collections import Counter
def count_words(lst):
  return dict(Counter(lst))

def good_turing_unigram(count_word,lst):
  turing_probabilities = {}
  total_count = len(lst)*1.
  number_unigrams_per_occurance = count_num_seen(count_word)
  for word in count_word:
    current_count = count_word[word]
    if number_unigrams_per_occurance[current_count+1]!=0:
      modified_count = (current_count +1)*number_unigrams_per_occurance[current_count] / number_unigrams_per_occurance[current_count+1]
    else:
      modified_count = current_count
    turing_probabilities[word] = modified_count/total_count
    print(turing_probabilities[word])
  return turing_probabilities

from collections import Counter
def count_num_seen(count_word):
  count = Counter(count_word.values())
  return count

def bicount_words(lst):
  modified_lst = [str(lst[i - 1]) + " " + str(lst[i]) for i in range (1,len(lst))]
  return dict(Counter(modified_lst))
  
def unsmoothed_bigram(lst):
  total_probabilities={}
  count_bigram=bicount_words(lst)
  count_unigram = count_words(lst)
  for words in count_bigram:
    total_probabilities[words]=count_bigram[words]/count_unigram[int(words.split(" ")[0])]
  return total_probabilities

def good_turing_bigram(count_unigram,count_word, unsmooth_bigram_dic):
  turing_probabilities = {}
  number_bigrams_per_occurance = count_num_seen(count_word)
  for word in count_word:
    current_count = count_word[word]
    if number_bigrams_per_occurance[current_count+1]!=0:
      modified_count = (current_count +1)*number_bigrams_per_occurance[current_count] / number_bigrams_per_occurance[current_count+1]
    else:
      modified_count = current_count
    turing_probabilities[word] = modified_count/count_unigram[int(word.split(" ")[0])]
    print(turing_probabilities[word])
  return turing_probabilities

# Your implementation here
# we expect a function or class, mapping a sequence of tokens to a sequence of labels
# this function or class will be called below
#tags are 0 for propaganda and 1 for not propaganda
#transition probabilities are calculated by bigram model
#emission probabilities are calculated by 
def transition_probabilities (data):
  lst_combined_types = []
  for article in data:
    lst_combined_types.extend(article[1])
  return good_turing_bigram(count_words(lst_combined_types),bicount_words(lst_combined_types),unsmoothed_bigram(lst_combined_types))

def convert_unk (data):
  modified_lst = []
  new_data=[]
  vocabulary_set = set()
  for article in data:
    lst,vocabulary = create_unknown_word_list(article[0])
    new_tuple = (lst,article[1],article[2])
    new_data.append(new_tuple)
    vocabulary_set.union(vocabulary)
  return new_data,vocabulary_set

unk_token_label_training,vocabulary_set = convert_unk(list_token_label_training)

transition_prob = transition_probabilities(list_token_label_training)

def emission_probabilities (data):
  modified_lst=[]
  lst_combined_types = []
  dict_probabilities={}
  for article in data:
    modified_lst.extend([article[0][i] + " " + str(article[1][i]) for i in range (1,len(article))])
    lst_combined_types.extend(article[1])
  count = count_words(modified_lst)
  count_types = count_words(lst_combined_types)
  for combination in count:
    dict_probabilities[combination]=count[combination]/count_types[int(combination.split(" ")[1])]
  return dict_probabilities

emission_prob = emission_probabilities(unk_token_label_training)

def start_transition_probabilities(data):
  lst_combined_types = []
  for article in data:
    lst_combined_types.extend(article[1])
  count = count_words(lst_combined_types)
  count0 = count[0]/(count[0]+count[1])
  count1 = count[1]/(count[0]+count[1])
  lst=[count0,count1]
  return lst

start_probs=start_transition_probabilities(unk_token_label_training)

def viterbi (test_article,transition_probs,emission_probs, start_transition_probs):
  start_prob0=start_transition_probs[0]
  start_prob1=start_transition_probs[1]
  str0 = test_article[0] + " 0"
  str1 = test_article[0] + " 1"
  score0 = start_prob0 * emission_probs[str0]
  score1 = start_prob1 * emission_probs[str1]
  backpointer=[0]
  for x in range(1,len(test_article)):  
    for y in range(1):
      stry = test_article[x] + " " + str(y)
      trans0 = "0 " + str(y)
      trans1 = "1 " + str(y)
      score0 = transition_probs[trans0] * score0 * emission_probs[stry]
      score1 = transition_probs[trans1] * score1 * emission_probs[stry]
      score=0
      if (score0<=score1):
        score=score1
        backpointer.append(1)
      else:
        score=score0
        backpointer.append(0)
  return backpointer

def unknown_word_article (article,vocab):
  article_with_unk = []
  for word in article:
    if word in vocab:
      article_with_unk.append(word)
    else:
      article_with_unk.append("<UNK>")  
  return article_with_unk

"""## Validation Step"""

# Evaluate/validate your model here
# you may attach pictures of graphs etc.

from sklearn.metrics import f1_score
viterbi_arrays = []
score = 0
for article in list_token_label_validation:
  article_with_unk = unknown_word_article(article[0],vocabulary_set)
  viterbi_array = viterbi(article_with_unk,transition_prob,emission_prob,start_probs)
  viterbi_arrays.append(viterbi_array)
  score+=f1_score(article[1], viterbi_array, average='weighted')
print(score/validation_length)

"""# Model 2: MEMM Implementation
"""

from sklearn.feature_extraction import DictVectorizer
from sklearn.linear_model import LogisticRegression
import nltk
import scipy
nltk.download('averaged_perceptron_tagger')

def fd_list_extraction (data):
  fd_list = []
  labels=[]
  for article in data:
    pos=nltk.pos_tag(article[0])
    labels.extend(article[1])
    for x in range(0,len(article[0])):
      if (x==0):
        dict_feature= {
            "length": len(article[0][x]),
            "prev_prev_word":"",
            "prev_word":"",
            "pos":pos[x][1],
            "prev_tag": 1
        }
      elif (x==1):
        dict_feature = {
          "length": len(article[0][x]),
          "prev_prev_word":"",
          "prev_word": article[0][x-1],
          "pos": pos[x][1],
          "prev_tag": article[1][x-1]
      }
      else:
        dict_feature = {
          "length": len(article[0][x]),
          "prev_prev_word":article[0][x-2],
          "prev_word": article[0][x-1],
          "pos": pos[x][1],
          "prev_tag": article[1][x-1]
      }
      fd_list.append(dict_feature)
  return fd_list,labels

# Your model implementation here
# we expect a function of class, mapping a sequence of tokens to a sequence of labels
# this function or class will be called below
#
# You will need:
# 1. Extract Features
# 2. Train MaxEnt
# 3. To call Viterbi

import scipy  
fd_list,labels=fd_list_extraction(list_token_label_training)
print(type(fd_list))
vec=DictVectorizer()
fd_list_vec=vec.fit_transform(fd_list)
clf=LogisticRegression(random_state=0,max_iter=1000).fit(fd_list_vec,labels)

print(fd_list[0])

print(clf)

coef = clf.coef_[0]
print (coef)

import math
import random
def viterbiMEMM (test_article,classifier,start_transition_probs):
  start_prob0=start_transition_probs[0]
  start_prob1=start_transition_probs[1]
  str0 = test_article[0] + " 0"
  str1 = test_article[0] + " 1"
  score0 = start_prob0
  score1 = start_prob1
  backpointer=[0]
  for x in range(1,len(test_article)): 
    r=random.randint(0,100)
    pos=nltk.pos_tag([test_article[x]])
    dict_feature= {
            "length": len(test_article[x]),
            "prev_prev_word":"" if x==1 else test_article[x-2],
            "prev_word":test_article[x-1],
            "pos":pos[0][1],
            "prev_tag":backpointer[x-1]
        }
    transform_vec = vec.transform(dict_feature)
    list_probs=classifier.predict_proba(transform_vec)
    for y in range(1):
      trans0 = "0 " + str(y)
      trans1 = "1 " + str(y)
      stry = test_article[x] + " " + str(y)
      score0 = list_probs[0][0] * score0
      score1 = list_probs[0][1] * score1
      score=0
      if (score0<=score1):
        score=score1
        backpointer.append(1)
      else:
        score=score0
        backpointer.append(0)
  return backpointer

# Run your model on validation set
# You will need to 
# 1. Call your function above to get a prediction result on Validation Set
# 2. Report Metrics
# (See if you need to modify your feature set)

from sklearn.metrics import f1_score
viterbi_arrays = []
score = []
labels=[]
for article in list_token_label_validation:
  viterbi_array = viterbiMEMM(article[0],clf,start_probs)
  viterbi_arrays.append(viterbi_array)
  score.append(f1_score(article[1], viterbi_array, average='weighted'))
  labels.append(article[1])
  # print(score)
print(viterbi_arrays[2])
print(labels[2])
